exp_name: nyt
run_name: TP2+Cat+Dist+BiLSTM

train_data: train_data.json
valid_data: valid_data.json
rel2id: rel2id.json

device_num: 0

# set logger
# if use default logger, must provide a log path and a path to save model, if use wandb, model state will be upload to the cloud
logger: wandb # wandb, default

# logger: default
# log_path: ./default.log
# path_to_save_model: ./model_state
 
# encoder: BERT
# data_home: ../data4bert
# bert_path: /home/wangyucheng/opt/transformers_models_h5/bert-base-cased

encoder: BiLSTM
token2idx: token2idx.json
data_home: ../data4bilstm
pretrained_word_embedding_path: ../pretrained_word_emb/glove_300_nyt.emb

hyper_parameters:
 batch_size: 24
 epochs: 200
 lr: 1e-3
 seed: 2333
 log_interval: 10
 max_seq_len: 100
 sliding_len: 20
 shaking_type: cat
 dist_emb_size: 512
 tok_pair_sample_rate: 1
 
 # CosineAnnealingWarmRestarts
 scheduler: CAWR # Step
 T_mult: 1
 rewarm_epoch_num: 2
 
#  # StepLR
#  scheduler: Step
#  decay_rate: 0.99
#  decay_steps: 100

#  scheduler: ReduceLROnPlateau
 
#  # for BiLSTM
#  enc_hidden_size: 128
#  dec_hidden_size: 256
#  emb_dropout: 0.1
#  rnn_dropout: 0.1
#  word_embedding_dim: 100

# whether train from scratch
fr_scratch: true
note: start from scratch
# when to save the model state dict
f1_2_save: 0.0

# if not, give a model_state_dict
model_state_dict_path: stake